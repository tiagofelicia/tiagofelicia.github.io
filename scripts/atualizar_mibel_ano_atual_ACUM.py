import pandas as pd
import numpy as np
import requests
import re
import io
import shutil
from datetime import datetime, timedelta
import os

# Detecta se o script est√° a correr dentro do GitHub Actions
RUNNING_IN_GITHUB = "GITHUB_ACTIONS" in os.environ

# ============================================================
# CONFIGURA√á√ïES
# ============================================================
FICHEIRO_MIBEL_CSV = "data/MIBEL_ano_atual_ACUM.csv"
FICHEIRO_MIBEL_XLS = "data/MIBEL_ano_atual_ACUM.xlsx"
DIAS_MINIMOS_ACUM = 365
BACKUP_SUFFIX = ".bak"

# ============================================================
# SISTEMA DE LOGS
# ============================================================
def header(msg): print(f"\nüîµ {msg}")
def log(msg): print(f"   - {msg}")
def sub(msg): print(f"       ‚Ä¢ {msg}")

# ============================================================
# FUN√á√ÉO: Extrair dados OMIE (DI√ÅRIO)
# ============================================================
def tentar_extrair_dados_omie_diario(data_inicio, data_fim):

    if data_inicio > data_fim:
        header("[DI√ÅRIO] Nenhum intervalo para preencher.")
        return pd.DataFrame()

    header(f"[DI√ÅRIO] A preencher buracos de {data_inicio} at√© {data_fim}")
    dias = pd.date_range(data_inicio, data_fim, freq="D")
    sub(f"{len(dias)} dia(s) a verificar‚Ä¶")

    headers = {"User-Agent": "Mozilla/5.0"}
    lista = []
    dias_futuros = 0
    dias_ok = 0
    dias_vazios = 0

    for d in dias:
        data_str = d.strftime('%Y%m%d')
        url = f"https://www.omie.es/es/file-download?parents=marginalpdbcpt&filename=marginalpdbcpt_{data_str}.1"

        try:
            r = requests.get(url, headers=headers, timeout=12)

            if r.status_code == 404:
                dias_futuros += 1
                continue

            r.raise_for_status()

            df = pd.read_csv(
                io.BytesIO(r.content),
                sep=';', skiprows=1, decimal=',', encoding='windows-1252',
                header=None, usecols=[3,4,5],
                names=['Hora','Preco_PT','Preco_ES']
            )

            df.dropna(inplace=True)
            df['Data'] = d.date()

            if df.empty:
                dias_vazios += 1
                continue

            dias_ok += 1
            lista.append(df)

        except Exception:
            dias_vazios += 1
            continue

    # -------- LOG FINAL DOS DI√ÅRIOS --------
    if dias_ok == 0:
        if dias_futuros + dias_vazios == len(dias):
            sub("‚ÑπÔ∏è Nenhum dia dispon√≠vel ‚Äî pode ser futuro ou falha OMIE.")
        return pd.DataFrame()

    sub(f"‚úÖ {dias_ok} dias recolhidos com sucesso.")
    return pd.concat(lista, ignore_index=True)

# ============================================================
# FUN√á√ÉO: Extrair dados LIVE (ACUM + INDICADORES)
# ============================================================
def tentar_extrair_dados_live():

    header("[LIVE] A extrair dados ACUM + INDICADORES")
    fontes = []
    df_acum = pd.DataFrame()
    df_ind = pd.DataFrame()

    # -------- ACUM --------
    try:
        sub("[ACUM] A descarregar‚Ä¶")

        df_acum = pd.read_csv(
            "https://www.omie.es/sites/default/files/dados/NUEVA_SECCION/INT_PBC_EV_H_ACUM.TXT",
            sep=';', skiprows=2, decimal=',', encoding='windows-1252',
            usecols=[0,1,2,3], names=['Data','Hora','Preco_ES','Preco_PT']
        )

        df_acum['Data'] = pd.to_datetime(df_acum['Data'], format="%d/%m/%Y", errors="coerce")
        df_acum.dropna(subset=['Data'], inplace=True)
        df_acum['Data'] = df_acum['Data'].dt.date
        fontes.append(df_acum)
        sub(f"[ACUM] OK ({len(df_acum)} registos)")

        # --- Intervalo de datas do ACUM ---
        try:
            data_min = df_acum['Data'].min()
            data_max = df_acum['Data'].max()
            sub(f"[ACUM] Intervalo temporal: {data_min} ‚Üí {data_max}")
        except:
            sub("[ACUM] Intervalo temporal: indispon√≠vel")
    except Exception as e:
        sub(f"[ACUM] ‚ùå Falhou ({e})")

    # -------- INDICADORES --------
    try:
        sub("[INDICADORES] A descarregar‚Ä¶")
        r = requests.get("https://www.omie.es/sites/default/files/dados/diario/INDICADORES.DAT", timeout=10)
        lines = r.content.decode("utf-8", errors="ignore").splitlines()

        sess = [l for l in lines if l.startswith("SESION;")][0]
        data_sessao = pd.to_datetime(sess.split(";")[1], format="%d/%m/%Y").date()

        dados = []
        for l in lines:
            m = re.match(r"^H(\d{1,2})Q([1-4]);", l)
            if not m: continue

            partes = l.split(";")
            hora_h = int(m.group(1))
            quarto = int(m.group(2))
            hora_seq = (hora_h - 1)*4 + quarto

            preco_es = float(partes[1].replace(",", "."))
            preco_pt = float(partes[2].replace(",", "."))

            dados.append([data_sessao, hora_seq, preco_pt, preco_es])

        df_temp_ind = pd.DataFrame(dados, columns=['Data','Hora','Preco_PT','Preco_ES'])
        
        # --- VALIDA√á√ÉO ROBUSTA DE LINHAS ---
        qtd_linhas = len(df_temp_ind)
        if qtd_linhas in [92, 96, 100]:
            df_ind = df_temp_ind
            fontes.append(df_ind)
            sub(f"[INDICADORES] OK, dia {data_sessao} ({qtd_linhas} registos)")
        else:
            sub(f"[INDICADORES] ‚ö†Ô∏è Ignorado: {qtd_linhas} registos (esperado 96/100/92).")
            # df_ind continua vazio
            
    except Exception as e:
        sub(f"[INDICADORES] ‚ùå Falhou ({e})")

    if not fontes:
        return pd.DataFrame(), 0, pd.DataFrame()

    df_live = pd.concat(fontes, ignore_index=True)
    num_dias_acum = df_acum['Data'].nunique() if len(df_acum) else 0

    return df_live, num_dias_acum, df_ind

# ============================================================
# FUN√á√ÉO: Ler hist√≥rico local
# ============================================================
def ler_historico_local():

    header("[HIST√ìRICO] A carregar hist√≥rico local")

    try:
        df = pd.read_csv(FICHEIRO_MIBEL_CSV, parse_dates=['Data'])
        df['Data'] = df['Data'].dt.date
        log(f"‚úîÔ∏è CSV carregado ({len(df)} registos)")
        return df
    except:
        log("‚ÑπÔ∏è CSV inexistente ‚Äî tentar XLS‚Ä¶")

    try:
        df = pd.read_excel(FICHEIRO_MIBEL_XLS)
        df['Data'] = pd.to_datetime(df['Data']).dt.date
        log(f"‚úîÔ∏è XLS carregado ({len(df)} registos)")
        return df
    except:
        log("‚ö†Ô∏è Nem CSV nem XLS dispon√≠veis ‚Äî hist√≥rico vazio.")
        return pd.DataFrame(columns=['Data','Hora','Preco_PT','Preco_ES'])

# ============================================================
# FUN√á√ÉO PRINCIPAL
# ============================================================
def run_update_historico():

    header("üöÄ A iniciar atualiza√ß√£o MIBEL")
    today = datetime.now().date()

    # ---- 1. Ler dados LIVE ----
    df_live, num_acum, df_ind = tentar_extrair_dados_live()
    
    # ---- 2. Ler hist√≥rico LOCAL (Sempre necess√°rio) ----
    df_local = ler_historico_local()

    # ---- 3. Definir a BASE ----
    if num_acum >= DIAS_MINIMOS_ACUM:
        header("[PLANO A] ACUM v√°lido ‚Äî usar como base principal")
        df_base = df_live.copy()
        df_base['Data'] = pd.to_datetime(df_base['Data']).dt.date
        
        # Se tivermos dados LOCAIS que s√£o mais recentes que o ACUM,
        # temos de os preservar para n√£o perder os dias "do meio" (o gap).
        if not df_base.empty and not df_local.empty:
            ultima_data_acum = df_base['Data'].max()
            
            # Filtrar dados locais que s√£o POSTERIORES ao ACUM
            df_gap_local = df_local[df_local['Data'] > ultima_data_acum].copy()
            
            if not df_gap_local.empty:
                log(f"‚ö†Ô∏è A recuperar {len(df_gap_local)} registos do local (gap entre ACUM e Hoje)...")
                log(f"   Dias recuperados: {df_gap_local['Data'].unique()}")
                df_base = pd.concat([df_base, df_gap_local], ignore_index=True)
        # -----------------------------
        
    else:
        header(f"[PLANO B] ACUM insuficiente ({num_acum} dias)")
        if not df_local.empty:
            df_base = df_local.copy()
        else:
            # Fallback se n√£o houver local nem ACUM suficiente
            df_base = df_live.copy() 
            if not df_base.empty:
                df_base['Data'] = pd.to_datetime(df_base['Data']).dt.date

    ultima = df_base['Data'].max() if not df_base.empty else today - timedelta(days=1)
    log(f"√öltima data na base consolidada: {ultima}")

    # ---- 4. Preencher buracos antigos (se necess√°rio) ----
    ind_date = df_ind['Data'].iloc[0] if not df_ind.empty else None

    if ind_date:
        log(f"INDICADORES dispon√≠vel para: {ind_date}")
        # Se temos indicador v√°lido, o buraco fecha ontem
        fim_buracos = ind_date - timedelta(days=1)
    else:
        # Se N√ÉO temos indicador, assumimos que queremos dados at√© amanh√£ 
        # (para tentar apanhar o di√°rio caso o indicador tenha falhado)
        fim_buracos = today + timedelta(days=1)

    df_diarios = pd.DataFrame()
    
    # S√≥ procuramos di√°rios se houver um buraco entre a Base e o Alvo
    if ultima < fim_buracos:
        df_diarios = tentar_extrair_dados_omie_diario(ultima + timedelta(days=1), fim_buracos)

    # ---- 5. Adicionar INDICADORES ----
    df_ind_to_add = pd.DataFrame()
    
    if ind_date:
        if ind_date > ultima:
            # Caso 1: O indicador √© NOVO (ainda n√£o est√° na base)
            df_ind_to_add = df_ind.copy()
            log(f"‚ûï A adicionar INDICADORES do dia {ind_date} (Novo)...")
            
        elif ind_date == ultima:
            # Caso 2: O indicador J√Å L√Å EST√Å (veio junto com o df_live no Plano A)
            log(f"‚úÖ O dia dos INDICADORES ({ind_date}) j√° foi integrado via Download Live.")
            
        else:
            # Caso 3: O indicador √© VELHO (ex: temos dia 5, indicador √© dia 1)
            log(f"‚ÑπÔ∏è O dia dos INDICADORES ({ind_date}) √© antigo e desnecess√°rio. Ignorado.")

    # ---- 6. Combinar tudo ----
    header("[FINAL] A combinar e guardar")
    dfs = [df_base]

    if not df_diarios.empty:
        dfs.append(df_diarios)

    if not df_ind_to_add.empty:
        dfs.append(df_ind_to_add)

    df_final = pd.concat(dfs, ignore_index=True)
    
    # ---- Normaliza√ß√£o FINAL do DataFrame ----

    # Hora num√©rica
    df_final['Hora'] = pd.to_numeric(df_final['Hora'], errors='coerce')

    # Pre√ßos: converter strings com v√≠rgulas para floats
    # (Garantir que √© string antes de replace para evitar erro em floats j√° convertidos)
    df_final['Preco_PT'] = df_final['Preco_PT'].astype(str).str.replace(',', '.', regex=False)
    df_final['Preco_PT'] = pd.to_numeric(df_final['Preco_PT'], errors='coerce')

    df_final['Preco_ES'] = df_final['Preco_ES'].astype(str).str.replace(',', '.', regex=False)
    df_final['Preco_ES'] = pd.to_numeric(df_final['Preco_ES'], errors='coerce')

    # Remover linhas com NaN cr√≠ticos
    df_final.dropna(subset=['Data', 'Hora', 'Preco_PT'], inplace=True)

    # ORDEM das colunas
    df_final = df_final[['Data','Hora','Preco_PT','Preco_ES']]

    # Ordena√ß√£o final e remo√ß√£o de duplicados exatos
    df_final = df_final.sort_values(['Data','Hora']).drop_duplicates(['Data','Hora'], keep='last')

    # BACKUP INTELIGENTE
    backup_path = FICHEIRO_MIBEL_CSV + BACKUP_SUFFIX

    if not RUNNING_IN_GITHUB:
        try:
            shutil.copy(FICHEIRO_MIBEL_CSV, backup_path)
            log(f"üíæ Backup criado (local): {backup_path}")
        except Exception as e:
            log(f"‚ö†Ô∏è Falha ao criar backup local: {e}")
    else:
        log("‚ÑπÔ∏è Backup ignorado (GitHub Actions)")

    df_final.to_csv(FICHEIRO_MIBEL_CSV, index=False, encoding='utf-8-sig', float_format="%.2f")

    log(f"‚úÖ Atualiza√ß√£o conclu√≠da: {len(df_final)} registos")
    log(f"   üìÖ Dados de: {df_final['Data'].min()} at√© {df_final['Data'].max()}")
    log("üèÅ FIM")

# ============================================================
# ENTRY
# ============================================================
if __name__ == "__main__":
    run_update_historico()
